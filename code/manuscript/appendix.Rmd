# Reproducibility

## Manuscript

We used `r sessionInfo()$R.version$version.string` to create this manuscript with the following packages:

```{r echo = FALSE, results = 'asis'}
# Grab only packages you explicitly loaded (attached packages)
my_pkgs <- sessioninfo::session_info()$packages %>% as.data.frame()
my_pkgs <- subset(my_pkgs, attached) %>% 
  select(package, loadedversion) 

apa_table(
  my_pkgs,
  colnames = c("Package", "Version"),
  caption = "R packages and versions used in the analyses.",
  note = "Versions correspond to the computational environment at the time of knitting.",
  placement = "H"
)
```

## Reproducible Model Code

The computational models reported in this manuscript can be fully reproduced using the scripts and notebooks provided in the project repository under `code/python_scripts/`. The primary modeling workflow is implemented in an IPython notebook, which documents the end-to-end processâ€”from data loading and preprocessing to model training, evaluation, and output generation.

To reproduce the models, users should create a compatible Python environment (see the provided requirements at the top of the notebook) and execute the notebook sequentially. The data and models created in this project are too large to share within GitHub, so we include a table below of how to obtain the word embeddings created in this project. The data downloads for Wikipedia and OpenSubtitles are provided within the python code. 

# Shiny App

COMING SOON

# Data 

You can find the word by dimensions embeddings using the following DOIs.  

WE ARE STILL WORKING ON THIS. 

```{r data-table, results='asis'}
df <- rio::import("dataset_list.xlsx") %>% 
  dplyr::arrange(Language)

papaja::apa_table(df, 
          caption = "DOIs for Word Embeddings", 
          col.names = c("Language", "Models", "DOI"))
```
