{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc10204",
   "metadata": {},
   "source": [
    "# word2manylanguages\n",
    "\n",
    "This experiment will test the assumption that disparate languages should be modeled using the same parameters, as in (van Paridon & Thompson, 2021). We will build 60 words-by-dimensions models for each language by systematically varying each parameter as described in the technical implementation below, and then test and rank the resulting models by their r-squared score. The best model for each language will be chosen, with best defined as the highest $R^2$ with the lowest window size and least dimensions.\n",
    "\n",
    "![word2manylanguages Process Flow](word2manylanguages_process.png)\n",
    "\n",
    "The fastText model (Bojanowski et al., 2016) from the gensim version 3.8.3 Python package (Řehůřek & Sojka, 2010) will be used to generate the embeddings from the concatenated corpus files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691e2e3f-734f-4929-bf0e-073da6656eb1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Required Packages\n",
    "\n",
    "- bz2\n",
    "- html\n",
    "- numpy as np\n",
    "- os\n",
    "- pandas as pd\n",
    "- re\n",
    "- requests\n",
    "- simhash\n",
    "- sklearn.linear_model\n",
    "- sklearn.model_selection\n",
    "- sklearn.preprocessing\n",
    "- sklearn.utils\n",
    "- zipfile\n",
    "- lxml import etree\n",
    "- gensim.models import FastText\n",
    "- glob\n",
    "- unicodedata\n",
    "- unidecode import unidecode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce99bc9",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368ace74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import word2manylanguages as w\n",
    "import os\n",
    "\n",
    "# Set the base directory for all subsequent operations. This should already exist.\n",
    "w.basedir = '../../data/processing_example'\n",
    "\n",
    "# Create the necessary output subdirectories if they don't already exist.\n",
    "for path in [w.datadir, w.processdir, w.corpusdir, w.modeldir, w.evaldir]:\n",
    "    output = os.path.join(w.basedir, path)\n",
    "    if not os.path.exists(output):\n",
    "        os.makedirs(output)\n",
    "\n",
    "# Set the list of languages to work on\n",
    "# Full list:\n",
    "# langs = ['af','ar','bg','bn','br','bs','ca','cs','da','de','el','en','eo','es','et',\n",
    "#         'eu','fa','fi','fr','gl','he','hi','hr','hu','hy','id','is','it','ja','ka',\n",
    "#         'kk','ko','lt','lv','mk','ml','ms','nl','no','pl','pt','ro','ru','si','sk',\n",
    "#         'sl','sq','sr','sv','ta','te','th','tl','tr','tw','uk','ur','vi','zh']\n",
    "\n",
    "langs = ['af']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0466c4bf",
   "metadata": {},
   "source": [
    "# Download Raw Data\n",
    "\n",
    "This download process is the original process at the time of the experiment.  While the Wikipedia download process still works as shown, the OpenSubtitles project has drastically changed their download procedures, making the data harder to obtain.  We will provide an alternative download that gives the same data the original process obtained.  \n",
    "\n",
    "Downloaded data will be stored in a subfolder under base directory called **data**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4254a7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remote file http://dumps.wikimedia.your.org/afwiki/latest/afwiki-latest-pages-meta-current.xml.bz2, Local file wikipedia-af.bz2\n",
      "File wikipedia-af.bz2 exists, and overwrite not specified. Skipping.\n",
      "Remote file https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2018/raw/af.zip, Local file subtitles-af.zip\n",
      "File subtitles-af.zip exists, and overwrite not specified. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# For each language, download the Wikipedia and OpenSubtitles data.\n",
    "for language in langs:\n",
    "    w.download('wikipedia', language)\n",
    "    w.download('subtitles', language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4902d4af",
   "metadata": {},
   "source": [
    "# Clean the Raw Data\n",
    "\n",
    "Perform data cleanup on each of the raw data files.\n",
    "\n",
    "Cleaned data will be stored in a subfolder under base directory called **preprocessed**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "014762f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File wikipedia-af-pre.zip exists, and overwrite not specified. Skipping.\n",
      "File subtitles-af-pre.zip exists, and overwrite not specified. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# For each language, perform data cleanup\n",
    "for language in langs:\n",
    "    w.clean('wikipedia', language)\n",
    "    w.clean('subtitles', language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0540a7",
   "metadata": {},
   "source": [
    "# Deduplicate the Cleaned Data\n",
    "\n",
    "This is an optional step that was ultimately skipped in the experiment because the data sources used here are well-curated and the likelihood of document duplication is low, and we feel that it serves our purposes better to include phrase duplication in order to accurately represent how common phrases such as \"Thank you\" are in spoken language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37df3705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sentence-level deduplication if desired\n",
    "#for language in langs:\n",
    "#    w.prune('wikipedia', language)\n",
    "#    w.prune('subtitles', language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3916ee",
   "metadata": {},
   "source": [
    "# Create the Corpus\n",
    "\n",
    "Concatenate the OpenSubtitles and Wikipedia data into a single corpus file, with one sentence per line.\n",
    "\n",
    "Concatenated data will be stored in a subfolder under base directory called **corpora**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8fe6403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File corpus-af.txt exists, and overwrite not specified. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# For each language, create the concatenated corpus\n",
    "for language in langs:\n",
    "    w.concatenate_corpus(language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554bb4b9",
   "metadata": {},
   "source": [
    "# Build the Words by Dimensions Models\n",
    "\n",
    "Using gensim, we create models with window sizes of 1, 2, 3, 4, 5, and 6 and dimensions of 50, 100, 200, 300, and 500. Each model is trained using the skip-gram or continuous bag of words algorithm. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4f64415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File af_50_1_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_50_1_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_50_2_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_50_2_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_50_3_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_50_3_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_50_4_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_50_4_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_50_5_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_50_5_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_50_6_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_50_6_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_100_1_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_100_1_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_100_2_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_100_2_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_100_3_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_100_3_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_100_4_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_100_4_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_100_5_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_100_5_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_100_6_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_100_6_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_200_1_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_200_1_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_200_2_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_200_2_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_200_3_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_200_3_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_200_4_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_200_4_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_200_5_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_200_5_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_200_6_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_200_6_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_300_1_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_300_1_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_300_2_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_300_2_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_300_3_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_300_3_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_300_4_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_300_4_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_300_5_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_300_5_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_300_6_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_300_6_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_500_1_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_500_1_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_500_2_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_500_2_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_500_3_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_500_3_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_500_4_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_500_4_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_500_5_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_500_5_sg_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_500_6_cbow_wxd.csv exists, and overwrite not specified. Skipping.\n",
      "File af_500_6_sg_wxd.csv exists, and overwrite not specified. Skipping.\n"
     ]
    }
   ],
   "source": [
    "# For each language, build models\n",
    "for language in langs:\n",
    "    w.build_models(language)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bcaca9",
   "metadata": {},
   "source": [
    "# Evaluate Norms with Replication Data Sets\n",
    "\n",
    "The replication datasets can be downloaded here: https://github.com/jvparidon/subs2vec/tree/master/subs2vec/datasets/norms \n",
    "\n",
    "For this example of processing, we created an `af-fake-2025.tsv` to show how the prediction code runs. We used the same code from the previous manuscript to calculate our prediction effect sizes. \n",
    "\n",
    "```    \n",
    "model = sklearn.linear_model.Ridge(alpha=alpha)  # use ridge regression models\n",
    "cv = sklearn.model_selection.RepeatedKFold(n_splits=5, n_repeats=10)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b615355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model af_500_1_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_500_1_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_500_2_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_500_2_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_500_3_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_500_3_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_500_4_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_500_4_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_500_5_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_500_5_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_500_6_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_500_6_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_300_1_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_300_1_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_300_2_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_300_2_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_300_3_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_300_3_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_300_4_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_300_4_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_300_5_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_300_5_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_300_6_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_300_6_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_200_1_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_200_1_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_200_2_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_200_2_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_200_3_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_200_3_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_200_4_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_200_4_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_200_5_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_200_5_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_200_6_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_200_6_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_100_1_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_100_1_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_100_2_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_100_2_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_100_3_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_100_3_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_100_4_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_100_4_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_100_5_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_100_5_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_100_6_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_100_6_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_50_1_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_50_1_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_50_2_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_50_2_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_50_3_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_50_3_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_50_4_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_50_4_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_50_5_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_50_5_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_50_6_cbow\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n",
      "Evaluating model af_50_6_sg\n",
      "predicting norms from af-fake-2025.tsv\n",
      "missing vectors for 1189 out of 152736 words\n"
     ]
    }
   ],
   "source": [
    "for language in langs:\n",
    "    w.loop_norms_vp(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd190b8a-6106-49f3-a7ae-be20a0317256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading eval ../../data/processing_example/evals/replication/af_100_3_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_200_1_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_50_5_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_500_4_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_500_4_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_50_6_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_300_2_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_50_2_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_100_5_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_50_5_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_50_1_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_200_4_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_500_3_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_500_1_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_100_4_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_50_3_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_300_3_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_500_2_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_300_4_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_100_6_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_200_4_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_300_1_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_200_3_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_100_3_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_50_6_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_200_2_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_300_4_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_200_6_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_50_1_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_200_5_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_200_2_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_300_6_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_300_5_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_300_1_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_100_4_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_300_6_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_300_3_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_100_1_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_300_2_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_200_5_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_50_4_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_500_5_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_200_1_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_200_3_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_50_4_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_500_6_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_500_6_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_100_5_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_100_2_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_100_1_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_500_2_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_100_6_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_500_5_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_500_1_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_100_2_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_300_5_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_50_3_cbow_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_200_6_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_50_2_sg_eval.csv\n",
      "Loading eval ../../data/processing_example/evals/replication/af_500_3_sg_eval.csv\n",
      "✅ Saved sorted results to ../../data/processing_example/scores/replication/af_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# put in which folder you want to combine datasets for \n",
    "for language in langs:\n",
    "    w.score_vp(language, \"replication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0eaaa4",
   "metadata": {},
   "source": [
    "# Evaluate Norms with Extended Data Sets\n",
    "\n",
    "The extended datasets can be downloaded from https://github.com/SemanticPriming/semanticprimeR/releases/tag/v0.0.1 - this release is periodically updated, but with datasets that could be added to the same prediction as below. \n",
    "\n",
    "We provide a synthetic data example in this folder `af-fake-2025.csv` to show reproducibility in the code. We created `datasets.csv` in the datasets folder to show the example, but you can see our original datasets file in `datasets_original.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b96ef40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model af_50_1_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_50_1_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_50_2_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_50_2_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_50_3_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_50_3_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_50_4_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_50_4_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_50_5_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_50_5_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_50_6_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_50_6_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_100_1_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_100_1_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_100_2_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_100_2_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_100_3_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_100_3_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_100_4_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_100_4_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_100_5_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_100_5_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_100_6_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_100_6_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_200_1_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_200_1_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_200_2_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_200_2_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_200_3_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_200_3_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_200_4_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_200_4_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_200_5_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_200_5_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_200_6_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_200_6_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_300_1_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_300_1_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_300_2_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_300_2_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_300_3_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_300_3_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_300_4_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_300_4_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_300_5_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_300_5_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_300_6_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_300_6_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_500_1_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_500_1_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_500_2_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_500_2_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_500_3_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_500_3_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_500_4_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_500_4_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_500_5_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_500_5_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_500_6_cbow\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n",
      "Loading model af_500_6_sg\n",
      "Evaluating af-fake-2025.csv\n",
      "Considering columns ['word', 'valence_M', 'arousal_M', 'familiar_M']\n",
      "missing vectors for 3 out of 114552 words\n"
     ]
    }
   ],
   "source": [
    "for language in langs:\n",
    "    w.evaluate_norms(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb2b71d1-347b-4437-8159-466462940854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading eval af_eval.csv\n",
      "    Language Dimensions Window Algorithm        Norm     Score\n",
      "199       af         50      4      cbow   arousal_M -0.000259\n",
      "19        af         50      4      cbow   arousal_M -0.000287\n",
      "181       af         50      1      cbow   arousal_M -0.000306\n",
      "31        af         50      6      cbow   arousal_M -0.000328\n",
      "22        af         50      4        sg   arousal_M -0.000339\n",
      "..       ...        ...    ...       ...         ...       ...\n",
      "173       af        500      5        sg  familiar_M -0.005536\n",
      "353       af        500      5        sg  familiar_M -0.005553\n",
      "359       af        500      6        sg  familiar_M -0.005583\n",
      "357       af        500      6        sg   valence_M -0.005818\n",
      "177       af        500      6        sg   valence_M -0.005955\n",
      "\n",
      "[360 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# put in which folder you want to combine datasets for \n",
    "for language in langs:\n",
    "    w.score_norms(language, \"norms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d813652",
   "metadata": {},
   "source": [
    "# Evaluate Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dcb7699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model af_50_1_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_50_1_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_50_2_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_50_2_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_50_3_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_50_3_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_50_4_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_50_4_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_50_5_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_50_5_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_50_6_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_50_6_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_100_1_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_100_1_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_100_2_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_100_2_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_100_3_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_100_3_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_100_4_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_100_4_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_100_5_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_100_5_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_100_6_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_100_6_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_200_1_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_200_1_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_200_2_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_200_2_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_200_3_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_200_3_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_200_4_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_200_4_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_200_5_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_200_5_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_200_6_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_200_6_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_300_1_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_300_1_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_300_2_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_300_2_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_300_3_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_300_3_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_300_4_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_300_4_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_300_5_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_300_5_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_300_6_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_300_6_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_500_1_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_500_1_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_500_2_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_500_2_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_500_3_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_500_3_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_500_4_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_500_4_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_500_5_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_500_5_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_500_6_cbow\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n",
      "Loading model af_500_6_sg\n",
      "Loading dedup.af.words.unigrams.tsv\n",
      "Cleaning dedup.af.words.unigrams.tsv\n",
      "Evaluating dedup.af.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 18348  matches: 20701\n",
      "missing vectors for -2353 out of 18348 words\n",
      "Loading dedup.afwiki-meta.words.unigrams.tsv\n",
      "Cleaning dedup.afwiki-meta.words.unigrams.tsv\n",
      "Evaluating dedup.afwiki-meta.words.unigrams.tsv\n",
      "vectors: 152735  freqs: 472483  matches: 139317\n",
      "missing vectors for 333166 out of 472483 words\n"
     ]
    }
   ],
   "source": [
    "for language in langs: \n",
    "    w.evaluate_counts(language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b199819-513c-4bfb-9c7f-cc24360ead6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading eval ../../data/processing_example/evals/counts/af_eval.csv\n",
      "✅ Saved sorted results to ../../data/processing_example/scores/counts/af_scores.csv\n"
     ]
    }
   ],
   "source": [
    "# put in which folder you want to combine datasets for \n",
    "for language in langs:\n",
    "    w.score_counts(language, \"counts\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Jupyter Env)",
   "language": "python",
   "name": "py311-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
