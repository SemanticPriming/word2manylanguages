{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquire and Clean Data\n",
    "  \n",
    "Data for building word vectors will be acquired from Open Subtitles and Wikipedia.  The data will be cleaned and formatted for processing with the following:  \n",
    "- No lemmatization or stemming  \n",
    "- Remove all formatting  \n",
    "- (Perhaps not) Remove documents where the 30 most frequent words did not cover at least 30% of the total number of tokens in that subtitle file  \n",
    "- Remove documents that are near duplicates  \n",
    " \n",
    "Data file names will be standardized locally to match the pattern {source}-{language}.{extension}\n",
    "The language portion of the name will use lower-case ISO-3166 country codes.  Wikipedia dumps are distributed using the bz2 compression format, while Open Subtitles uses Zip compression. \n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download(source, language): \n",
    "    \"\"\"\n",
    "    Download data by source and language.  \n",
    "    Source must be one of {'subtitles', 'wikipedia'}.\n",
    "    Language must be a valid ISO3166 country code (lower case)\n",
    "    \n",
    "    Output file will be named in the pattern 'source-language.extension'.  Subtitle files use the 'zip' extension, while\n",
    "    Wikipedia dumps use 'bz2'.  For example, download('subtitles', 'fr') will result in a file called 'subtitles-fr.zip'\n",
    "    \"\"\"\n",
    "    sources = {\n",
    "                'subtitles': f'http://opus.nlpl.eu/download.php?f=OpenSubtitles/v2018/raw/{language}.zip',\n",
    "                'wikipedia': f'http://dumps.wikimedia.your.org/{language}wiki/latest/{language}wiki-latest-pages-meta-current.xml.bz2'\n",
    "    }\n",
    "    extensions = {\n",
    "        'subtitles': 'zip',\n",
    "        'wikipedia': 'bz2'\n",
    "    }\n",
    "    file_name = f'{source}-{language}.{extensions[source]}'\n",
    "    print(f'Remote file {sources[source]}, Local file {file_name}')\n",
    "    \n",
    "    r = requests.get(sources[source], stream=True)\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024):\n",
    "            f.write(chunk)\n",
    "    print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test download using English subtitles\n",
    "download('subtitles', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test download using English wikipedia\n",
    "download('wikipedia', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File reading utilities to deal with the fact that the raw data files are very big.\n",
    "import bz2\n",
    "import html\n",
    "\n",
    "class sentences(object):\n",
    "    \"\"\"\n",
    "    Return lines from a full corpus text file as a sequence\n",
    "    using the generator pattern (an iterable)\n",
    "    \"\"\"\n",
    "    def __init__(self, language):\n",
    "        self.myfile = open(f'corpus-{language}.txt', 'r')\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    " \n",
    "    # Python 3 compatibility\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    " \n",
    "    def next(self):\n",
    "        line = self.myfile.readline()\n",
    "        if line:\n",
    "            tok = [w for w in line.rstrip().split(' ') if len(w) > 0] \n",
    "            return tok\n",
    "        else:\n",
    "            self.myfile.close()\n",
    "            raise StopIteration()\n",
    "            \n",
    "class articles(object):\n",
    "    \"\"\"\n",
    "    Read a wikipedia dump file and return one article at a time\n",
    "    using the generator pattern (an iterable)\n",
    "    \"\"\"\n",
    "    def __init__(self, language):\n",
    "        self.myfile = bz2.open(f'wikipedia-{language}.bz2', 'rt', encoding='utf-8')\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    " \n",
    "    # Python 3 compatibility\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    " \n",
    "    def next(self):\n",
    "        article = \"\"\n",
    "        body = False\n",
    "        line = self.myfile.readline()\n",
    "        while line:\n",
    "            if \"<page>\" in line:\n",
    "                body = True\n",
    "            \n",
    "            if \"</page>\" in line:\n",
    "                return html.unescape(html.unescape(article))    \n",
    "            \n",
    "            if body:\n",
    "                article = article + line\n",
    "            \n",
    "            line = self.myfile.readline()\n",
    "            \n",
    "        self.myfile.close()\n",
    "        raise StopIteration()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import zipfile\n",
    "import os\n",
    "import re\n",
    "from lxml import etree\n",
    "import simhash\n",
    "\n",
    "def clean(source, language):\n",
    "    \"\"\"\n",
    "    Start the cleaning process for a given source and language.  Routes to the appropriate file handing\n",
    "    functions for the given source.\n",
    "    \"\"\"\n",
    "    \n",
    "    if ('subtitles' == source):\n",
    "        clean_subtitles(language)\n",
    "        prune(source, language)\n",
    "    else:\n",
    "        clean_wikipedia(language)\n",
    "        prune(source, language)\n",
    "\n",
    "        \n",
    "def sub_xml_to_text(xml, parser):\n",
    "    tree = etree.fromstring(xml, parser)\n",
    "    for node in tree.iter():\n",
    "        if node.tag == 'meta':\n",
    "            tree.remove(node)\n",
    "    return etree.tostring(tree, encoding=str, method='text')\n",
    "\n",
    "def wiki_strip_circumflex(txt):\n",
    "    circumflex = 0\n",
    "    txt = list(txt)\n",
    "    for i in range(len(txt)):\n",
    "        if txt[i] == '{':\n",
    "            circumflex += 1\n",
    "        elif txt[i] == '}':\n",
    "            circumflex -= 1\n",
    "            txt[i] = ''\n",
    "        if circumflex > 0:\n",
    "            txt[i] = ''\n",
    "        elif circumflex < 0:\n",
    "            # discard unmatched\n",
    "            txt = []\n",
    "            break\n",
    "    return ''.join(txt)\n",
    "\n",
    "\n",
    "subs_expressions = [\n",
    "    (r'<.*?>', ''),  # xml tags\n",
    "    (r'http.*?(?:[\\s\\n\\]]|$)', ''),  # links\n",
    "    (r'\\s\\(.*?\\)', ''),  # parentheses\n",
    "    (r'([^\\s]{2,})[\\.\\!\\?\\:\\;]+?[\\s\\n]|$', '\\\\1\\n'),  # break sentences at periods\n",
    "    (r\"[-–—/']\", ' '),  # hyphens, apostrophes and slashes\n",
    "    (r'\\s*\\n\\s*', '\\n'),  # empty lines\n",
    "    (r'\\s{2,}', ' '),  # excessive spaces\n",
    "]\n",
    "subs_patterns = [(re.compile(expression[0], re.IGNORECASE), expression[1]) for expression in subs_expressions]\n",
    "\n",
    "wiki_expressions = [\n",
    "    (r'(?s)<ref.*?</ref>', ''),  # strip reference links\n",
    "    (r'(?s)<references.*?</references>', ''),  # strip references\n",
    "    (r'(?s)<table.*?</table>', ''),  # strip tables\n",
    "    (r'(?s)<gallery.*?</gallery>', ''),  # strip galleries\n",
    "    (r'(?s)<kml.*?</kml>', ''),  # strip KML tags\n",
    "    (r'<.*?>', ''),  # strip other xml tags\n",
    "    (r'http.*?(?:[\\s\\n\\]]|$)', ''),  # strip external http(s) links\n",
    "    (r'\\[\\[[^\\]]*?:.*\\|(.*?)\\]\\]', '\\\\1'),  # strip links to files, etc. but keep labels\n",
    "    (r'\\[\\[[^\\]]*?:(.*?)\\]\\]', ''),  # strip category links\n",
    "    (r'\\[\\[[^\\]]*?\\|(.*?)\\]\\]', '\\\\1'),  # convert labeled links to just labels\n",
    "    (r'(?m)^[\\s]*[!?*;:=+\\-|#_].*?$', ''),  # strip lines that do not start with alphanumerics, quotes, or brackets\n",
    "    (r'(?m)^.*?\\(UTC\\).*?$', ''),  # strip lines containing a time stamp\n",
    "    (r'\\s\\(.*?\\)', ''),  # remove everything in parentheses\n",
    "    (r'([^\\s.!?:;]{2})[.!?:;]+?[\\s\\n]|$', '\\\\1\\n'),  # break sentences at periods\n",
    "    (r\"[-–—/']\", ' '),  # replace hyphens, apostrophes and slashes with spaces\n",
    "    (r'\\s*\\n\\s*', '\\n'),  # strip empty lines and lines containing whitespace\n",
    "    (r'\\s{2,}', ' '),  # strip excessive spaces\n",
    "]\n",
    "\n",
    "wiki_patterns = [(re.compile(expression[0], re.IGNORECASE), expression[1]) for expression in wiki_expressions]\n",
    "\n",
    "def clean_text(text, patterns):\n",
    "    txt = text\n",
    "    for pattern in patterns:\n",
    "        txt = pattern[0].sub(pattern[1], txt)\n",
    "    txt = ''.join([letter for letter in txt if (letter.isalnum() or letter.isspace())]) \n",
    "    return txt.lower() # Possibly lower-case here\n",
    "    \n",
    "def clean_subtitles(language):\n",
    "    \"\"\"\n",
    "    Prepare subtitle files for processing.\n",
    "    \"\"\"\n",
    "    input_file = zipfile.ZipFile(f'subtitles-{language}.zip', 'r')\n",
    "    output_file = zipfile.ZipFile(f'subtitles-{language}-pre.zip', 'a', zipfile.ZIP_DEFLATED)\n",
    "    \n",
    "    xmlparser = etree.XMLParser(recover=True, encoding='utf-8')\n",
    "    \n",
    "    # Make list of files to process\n",
    "    files = []\n",
    "    for f in input_file.namelist():\n",
    "        if f.endswith('xml'):\n",
    "            if f.startswith(os.path.join('OpenSubtitles/raw', language)):\n",
    "                files.append(f)\n",
    "    print(f'Preprocessing {len(files)} {language} subtitle files.')\n",
    "    for f in sorted(files):\n",
    "        output_file.writestr(f.replace('xml', 'txt'),\n",
    "                             clean_text(sub_xml_to_text(input_file.open(f).read(), xmlparser), subs_patterns))\n",
    "    print('Complete')\n",
    "    \n",
    "    \n",
    "def token_frequency_check(tokens):\n",
    "    \"\"\"\n",
    "    Checking to see if the 30 most frequent tokens cover 30% of all tokens.\n",
    "    Probably not doing this.\n",
    "    \"\"\"\n",
    "    s = set(tokens)\n",
    "    freqs = []\n",
    "    for t in s:\n",
    "        freqs.append((t, tokens.count(t)))\n",
    "\n",
    "    freqs.sort(key = lambda x: x[1])\n",
    "    \n",
    "    thresh = 30\n",
    "    if len(freqs) < 30:\n",
    "        thresh = len(freqs)\n",
    "    t30 = 0\n",
    "    for i in range(thresh):\n",
    "        t30 += freqs[i][1]\n",
    "    return t30 >= len(tokens) * 0.3\n",
    "    \n",
    "def get_hash(tokens):\n",
    "    shingles = [''.join(shingle) for shingle in\n",
    "                    simhash.shingle(''.join(tokens), 4)]\n",
    "    hashes = [simhash.unsigned_hash(s.encode('utf8')) for s in shingles]\n",
    "    return simhash.compute(hashes)\n",
    "\n",
    "def prune(source, language):\n",
    "    \"\"\"\n",
    "    Remove duplicate documents from subtitle file.\n",
    "    \"\"\"\n",
    "    input_file = zipfile.ZipFile(f'{source}-{language}-pre.zip', 'r')\n",
    "    output_file = zipfile.ZipFile(f'{source}-{language}-pruned.zip', 'a', zipfile.ZIP_DEFLATED)\n",
    "\n",
    "    to_remove = []\n",
    "    hash_list = []\n",
    "    hash_dict = dict()\n",
    "    \n",
    "    print(\"Checking for duplicates.\")\n",
    "    for f in input_file.namelist():\n",
    "        text = str(input_file.open(f).read())\n",
    "        tokens = re.split(r'\\W+', text.lower(), flags=re.UNICODE)\n",
    "        hash = get_hash(tokens)\n",
    "        hash_list.append(hash)\n",
    "        hash_dict[hash] = f\n",
    "            \n",
    "    blocks = 4\n",
    "    distance = 2\n",
    "    matches = simhash.find_all(hash_list, blocks, distance)\n",
    "    print(f'Got {len(matches)} matches')\n",
    "    for match in matches:\n",
    "        print(f'({hash_dict[match[0]]}, {hash_dict[match[1]]})')\n",
    "        to_remove.append(hash_dict[match[1]])\n",
    "    \n",
    "    print(f'Found {len(to_remove)} files to prune.')\n",
    "    \n",
    "    for f in input_file.namelist():\n",
    "        if f not in to_remove:\n",
    "            output_file.writestr(f, input_file.open(f).read())\n",
    "\n",
    "    output_file.close()\n",
    "    \n",
    "    \n",
    "def concatenate_corpus(language):\n",
    "    subs_input_file = zipfile.ZipFile(f'subtitles-{language}-pruned.zip', 'r')\n",
    "    wiki_input_file = zipfile.ZipFile(f'wikipedia-{language}-pruned.zip', 'r')\n",
    "    output_corpus = f'corpus-{language}.txt'\n",
    "    with open(output_corpus, mode=\"w\") as out:\n",
    "        for f in subs_input_file.namelist():\n",
    "            out.write(subs_input_file.open(f).read().decode(\"utf-8\"))\n",
    "        for f in wiki_input_file.namelist():\n",
    "            out.write(wiki_input_file.open(f).read().decode(\"utf-8\"))\n",
    "\n",
    "    \n",
    "    \n",
    "def clean_wikipedia(language):\n",
    "    \"\"\"\n",
    "    Prepare wikipedia files for processing.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(f'wikipedia-{language}-pre.zip', 'a', zipfile.ZIP_DEFLATED) as output_archive:\n",
    "        i = 0\n",
    "        print(f'Preprocessing {language} Wikipedia dump.')\n",
    "        for article in articles(language):\n",
    "            filename = f'wiki-{language}-{str(i)}.txt'\n",
    "            txt = article.lower()\n",
    "            txt = wiki_strip_circumflex(article) if ((not txt.startswith('#'))\n",
    "                                 and ('<noinclude>' not in txt)\n",
    "                                 and ('__noindex__' not in txt)\n",
    "                                 ) else ''\n",
    "            for pattern in wiki_patterns:\n",
    "                txt = pattern[0].sub(pattern[1], txt)\n",
    "        \n",
    "            output_archive.writestr(filename, ''.join([letter for letter in txt if (letter.isalnum() or letter.isspace())]))\n",
    "            i += 1\n",
    "            \n",
    "        print(\"Complete\")\n",
    "\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cleaning English subtitles\n",
    "clean('subtitles', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing en Wikipedia dump.\n"
     ]
    }
   ],
   "source": [
    "# Test cleaning English wikipedia\n",
    "clean('wikipedia', 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate_corpus('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec Method\n",
    "Use gensim\n",
    "Minimum frequency is 5\n",
    "Number of dimensions: 50, 100, 200, 300, and 500\n",
    "Negative sampling k was 10\n",
    "Sub-sampling parameter to 1e-5\n",
    "Window size was 3 to 13 (i.e. 1 word before and after to 10 words before and after)\n",
    "CBOW and Skip Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "\n",
    "def vectorize_stream(language, min_freq=5, dim=50, win=3, alg=0): \n",
    "    model = Word2Vec(sentences(language), min_count=min_freq, size=dim, workers=3, window=win, sg=alg)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_en_50_3_cbow = vectorize_stream('en', dim=50, win=3, alg=0)\n",
    "words=list(model_en_50_3_cbow.wv.vocab)\n",
    "wordsXdims_en_50_3_cbow = pd.DataFrame(model_en_50_3_cbow[words],words) \n",
    "\n",
    "#print(wordsXdims_en_50_3_cbow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_en_100_3_cbow = vectorize_stream('en', dim=100, win=3, alg=0)\n",
    "words=list(model_en_100_3_cbow.wv.vocab)\n",
    "wordsXdims_en_100_3_cbow = pd.DataFrame(model_en_100_3_cbow[words],words) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis Method\n",
    "See example of how to get out word vector for words by dimensions (https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751)\n",
    "Grab words by dimensions\n",
    "Use linear regression to predict Y (varies by language) given the word2vec dimensions\n",
    "Potentially also consider using ridge regression to control for multicollinearity of the words by dimensions matrix\n",
    "Pick model with the highest R2 values (DECISION HERE: what is the best model? What about ties?)\n",
    "\n",
    "Best Model:  Highest R-squared within 1% buckets\n",
    "             Lowest number of dimensions\n",
    "             Smallest window\n",
    "             Do we care about CBOW vs Skip-gram if there's a tie in the above 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import sklearn.utils\n",
    "import os\n",
    "\n",
    "def evaluate_norms(lang, wordsXdims, alpha=1.0):\n",
    "    # Using subs2vec norms data for now\n",
    "    path = os.path.join('/', 'home', 'pgrim', 'workspace', 'subs2vec', 'subs2vec')\n",
    "    norms_path = os.path.join(path, 'datasets', 'norms')\n",
    "\n",
    "    scores = []\n",
    "    for norms_fname in os.listdir(norms_path):\n",
    "        if norms_fname.startswith(lang):\n",
    "            print(f'predicting norms from {norms_fname}')\n",
    "            norms = pd.read_csv(os.path.join(norms_path, norms_fname), sep='\\t', comment='#')\n",
    "            norms = norms.set_index('word')\n",
    "            score = predict_norms(wordsXdims, norms, alpha)\n",
    "            score['source'] = norms_fname\n",
    "            scores.append(score)\n",
    "    \n",
    "    if len(scores) > 0:\n",
    "        scores = pd.concat(scores)\n",
    "        return scores\n",
    "\n",
    "def predict_norms(vectors, norms, alpha=1.0):\n",
    "    cols = norms.columns.values\n",
    "    df = norms.join(vectors, how='inner')\n",
    "    \n",
    "    # compensate for missing ys somehow\n",
    "    total = len(norms)\n",
    "    missing = len(norms) - len(df)\n",
    "    penalty = (total - missing) / total\n",
    "    print(f'missing vectors for {missing} out of {total} words')\n",
    "    df = sklearn.utils.shuffle(df)  # shuffle is important for unbiased results on ordered datasets!\n",
    "\n",
    "    model = sklearn.linear_model.Ridge(alpha=alpha)  # use ridge regression models\n",
    "    cv = sklearn.model_selection.RepeatedKFold(n_splits=5, n_repeats=10)\n",
    "\n",
    "    # compute crossvalidated prediction scores\n",
    "    scores = []\n",
    "    for col in cols:\n",
    "        # set dependent variable and calculate 10-fold mean fit/predict scores\n",
    "        df_subset = df.loc[:, vectors.columns.values]  # use .loc[] so copy is created and no setting with copy warning is issued\n",
    "        df_subset[col] = df[col]\n",
    "        df_subset = df_subset.dropna()  # drop NaNs for this specific y\n",
    "        x = df_subset[vectors.columns.values]\n",
    "        y = df_subset[col]\n",
    "        cv_scores = sklearn.model_selection.cross_val_score(model, x, y, cv=cv)\n",
    "        median_score = np.median(cv_scores)\n",
    "        penalized_score = median_score * penalty\n",
    "        scores.append({\n",
    "            'norm': col,\n",
    "            'adjusted r': np.sqrt(penalized_score),  # take square root of explained variance to get Pearson r\n",
    "            'adjusted r-squared': penalized_score,\n",
    "            'r-squared': median_score,\n",
    "            'r': np.sqrt(median_score),\n",
    "        })\n",
    "    return pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores = evaluate_norms('en', wordsXdims_en_50_3_cbow)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = evaluate_norms('en', wordsXdims_en_100_3_cbow)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide Output\n",
    "Include code to recreate these models\n",
    "Include word by dimension output as a data object\n",
    "Include functions to calculate cosine/other similarity measures on the word by dimension data object (if python, we could us the many measures in scipy.spatial.distance)\n",
    "Note that gensim's most_similar function provides the highest cosine on word by dimension data object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
